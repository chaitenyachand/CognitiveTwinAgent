<!-- agora_rtc.html -->
<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Cognitive Twin — Agora Conversational Tutor</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body { font-family: Inter, system-ui, -apple-system, "Segoe UI", Roboto, Arial; background:#f7fafc; color:#0f172a; padding:18px;}
    .panel { max-width: 1000px; margin: 0 auto; background:white; padding:18px; border-radius:12px; box-shadow:0 6px 20px rgba(2,6,23,0.06);}
    .controls { display:flex; gap: 8px; align-items:center; margin-bottom:12px;}
    .status { display:inline-block; padding:6px 10px; border-radius:999px; font-weight:600; color:white; background:#94a3b8; }
    .status.listening { background:#10b981; }     /* green */
    .status.thinking { background:#f59e0b; }      /* amber */
    .status.speaking { background:#6366f1; }      /* purple */
    .transcript { min-height:120px; border:1px dashed #e2e8f0; padding:12px; border-radius:8px; margin-bottom:12px; white-space:pre-wrap; }
    .assistant { margin-top: 12px; padding:12px; border-radius:8px; background: linear-gradient(90deg,#eef2ff,#f0f9ff); }
    .small { font-size:0.85rem; color:#64748b; }
  </style>
</head>
<body>
  <div class="panel">
    <h2>Voice-First: Cognitive Twin (Agora Conversational)</h2>
    <p class="small">This page uses Agora Conversational SDK for production-grade STT & TTS. Make sure Conversational is enabled in your Agora Console.</p>

    <div class="controls">
      <input id="channelName" placeholder="Channel name (e.g. chaitenya_channel)" />
      <input id="sessionTopic" placeholder="Topic (for DB session)" />
      <button id="joinBtn">Start Voice Tutor</button>
      <button id="leaveBtn" disabled>End Session</button>
      <div style="margin-left: auto;">
        <span id="stateBadge" class="status">idle</span>
      </div>
    </div>

    <div class="transcript" id="transcript">Transcript will appear here...</div>

    <h4>Assistant Reply</h4>
    <div class="assistant" id="assistantReply">Assistant responses will appear here.</div>
    <div style="margin-top:12px;">
      <small class="small">This implementation uses full Agora Conversational SDK for STT/TTS. If you want to use browser TTS/STT fallback, we can keep that as a dev fallback.</small>
    </div>
  </div>

  <!-- ==== IMPORTANT: Replace the following script with the official Agora Conversational SDK URL ==== -->
  <!-- Example placeholder (not a real conversational CDN) -->
  <!-- <script src="https://cdn.agora.io/sdk/conversational/conversational-sdk.min.js"></script> -->
  <!-- Check Agora docs and paste the conversational SDK script link here. -->

  <!-- For RTC functionality (audio publishing), we still use AgoraRTC -->
  <script src="https://download.agora.io/sdk/release/AgoraRTC_N-4.8.0.js"></script>

  <script>
    const tokenServer = window.location.origin.replace(/:\\d+$/, ':8000'); // assumes token_server on port 8000
    let agoraClient = null;
    let convClient = null; // conversational client placeholder
    let localAudioTrack = null;
    let joined = false;
    let sessionId = null;

    const stateBadge = document.getElementById("stateBadge");
    function setState(s) {
      stateBadge.textContent = s;
      stateBadge.classList.remove("listening","thinking","speaking");
      if (s === "listening") stateBadge.classList.add("listening");
      if (s === "thinking") stateBadge.classList.add("thinking");
      if (s === "speaking") stateBadge.classList.add("speaking");
    }

    // UI helpers
    const transcriptEl = document.getElementById("transcript");
    const assistantEl = document.getElementById("assistantReply");
    function appendTranscript(text) {
      transcriptEl.textContent = transcriptEl.textContent + (transcriptEl.textContent ? "\\n" : "") + text;
    }
    function setAssistant(text) {
      assistantEl.textContent = text;
    }

    async function fetchToken(channel, uid="0") {
      const resp = await fetch(`${tokenServer}/token?channel=${encodeURIComponent(channel)}&uid=${uid}`);
      return resp.json();
    }

    // Create voice session in your DB via token_server (simple approach: call your existing backend or DB API)
    async function createVoiceSession(user_id, topic) {
      // This is app-specific: the simplest approach is to call Streamlit endpoint (if you expose one),
      // or call your own backend. For now we call the token server to defer creation to agentic endpoints.
      // You should implement an endpoint to create a voice_session and return session_id.
      // We'll simply return null (client will still work), but it's better to implement create endpoint.
      return null;
    }

    document.getElementById("joinBtn").onclick = async () => {
      const channel = document.getElementById("channelName").value || "default_channel";
      const topic = document.getElementById("sessionTopic").value || channel;
      setState("listening");
      // 1) get tokens
      const tokenPayload = await fetchToken(channel, "0");
      const APP_ID = tokenPayload.app_id;
      const TOKEN = tokenPayload.token;

      // 2) join Agora RTC to publish mic
      agoraClient = AgoraRTC.createClient({ mode: "rtc", codec: "vp8" });
      await agoraClient.join(APP_ID, channel, TOKEN || null, 0);
      localAudioTrack = await AgoraRTC.createMicrophoneAudioTrack();
      await agoraClient.publish([localAudioTrack]);

      // 3) Create voice session in DB (optional)
      sessionId = await createVoiceSession(null, topic);

      // 4) Initialize Conversational client (SDK-specific)
      // ==== START: Conversational SDK integration (SDK-specific API below) ====
      // PSEUDOCODE — replace with real Conversational SDK methods per Agora docs:
      //
      // convClient = await AgoraConversational.createClient({ appId: APP_ID, token: CONV_TOKEN });
      // await convClient.join({ channel, uid: <uid> });
      // convClient.on('transcription', handleTranscriptionEvent);
      // convClient.on('response', handleAssistantResponse);
      // convClient.startListening(); // depends on SDK
      //
      // The code above should stream audio to Agora Conversational service, get real-time transcripts,
      // and receive AI-generated TTS audio back from Agora. Use convClient.speak() or SDK TTS play function.
      //
      // ==== END Conversational SDK integration placeholder ====

      // For now: we will rely on local browser SpeechRecognition ONLY if you don't add Conversational SDK.
      // But since you asked to replace browser STT/TTS with Conversational SDK, integrate the real SDK here.
      //
      // 5) Hook up placeholder handlers (you will replace with real SDK events):
      window.handleTranscription = async (transcript, isFinal) => {
        // send partials to server for analytics
        try {
          await fetch(`${tokenServer}/log_partial`, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              user_id: null,
              session_id: sessionId,
              topic: topic,
              partial_transcript: transcript,
              timestamp: Date.now() / 1000
            })
          });
        } catch (e) {
          console.warn("Partial log failed", e);
        }

        if (isFinal) {
          setState("thinking");
          // send final transcript to /process_transcript
          try {
            const resp = await fetch(`${tokenServer}/process_transcript`, {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({
                user_id: null,
                session_id: sessionId,
                topic: topic,
                transcript: transcript,
                persona: "empathetic",
                partial: false
              })
            });
            const data = await resp.json();
            const ai_reply = data.ai_reply || (data.structured && data.structured.ai_reply) || "Sorry, no response.";
            setAssistant(ai_reply);

            // If Conversational SDK supports server-side TTS playback, call convClient.playTTS(ai_reply)
            // Placeholder: if convClient has tts, call it; otherwise fallback to speechSynthesis (dev)
            if (window.convClient && convClient.playTTS) {
              setState("speaking");
              await convClient.playTTS(ai_reply);
              setState("listening");
            } else {
              // Fallback browser TTS (dev mode)
              setState("speaking");
              speakText(ai_reply).then(()=> setState("listening"));
            }
          } catch (e) {
            console.error("Error processing transcript:", e);
            setAssistant("Error contacting server: " + e.toString());
            setState("listening");
          }
        } else {
          appendTranscript("[partial] " + transcript);
        }
      };

      // update UI
      joined = true;
      document.getElementById("joinBtn").disabled = true;
      document.getElementById("leaveBtn").disabled = false;
      appendTranscript("Joined channel: " + channel);
    };

    document.getElementById("leaveBtn").onclick = async () => {
      // end session: leave agora and conv clients
      try {
        if (localAudioTrack) {
          localAudioTrack.stop();
          localAudioTrack.close();
        }
        if (agoraClient) {
          await agoraClient.leave();
        }
        if (convClient && convClient.leave) {
          await convClient.leave();
        }
      } catch (e) {
        console.warn("Error leaving", e);
      }
      setState("idle");
      joined = false;
      document.getElementById("joinBtn").disabled = false;
      document.getElementById("leaveBtn").disabled = true;
      appendTranscript("Left channel");
    };

    // dev fallback TTS
    function speakText(text) {
      return new Promise((resolve, reject) => {
        try {
          const utt = new SpeechSynthesisUtterance(text);
          utt.onend = () => resolve();
          utt.onerror = (e)=> reject(e);
          window.speechSynthesis.cancel();
          window.speechSynthesis.speak(utt);
        } catch (e) {
          resolve();
        }
      });
    }
  </script>
</body>
</html>
